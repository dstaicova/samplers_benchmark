{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796dedf-3082-4b2f-9aa2-f2d9427d404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file for the simple test cases used in the paper. \n",
    "#Please cite the paper, if you use the code or parts of it, as writing the code took a lot of time and effort. \n",
    "#Staicova D., Universe 2025, 11(2), 68; https://doi.org/10.3390/universe11020068, arXiv:2501.06022\n",
    "# !in order to reproduce the plots, you need to run it as the github version doesn't load the original results\n",
    "import numpy as np\n",
    "import emcee\n",
    "import pymc as pm\n",
    "import numpyro\n",
    "import dynesty\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import time\n",
    "import arviz as az\n",
    "import traceback\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "import threading\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import time\n",
    "import pymc as pm\n",
    "import emcee\n",
    "import numpyro\n",
    "import dynesty\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "import arviz as az\n",
    "import memory_profiler\n",
    "from functools import wraps\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "class TestProblems:\n",
    "    @staticmethod\n",
    "    def correlated_gaussian(ndim):\n",
    "        \"\"\"Correlated Gaussian test problem\"\"\"\n",
    "        def log_like(x):\n",
    "            # Check if input is a PyMC variable\n",
    "            if hasattr(x, 'type'):\n",
    "                # PyMC version\n",
    "                import pymc as pm\n",
    "                return -0.5 * pm.math.sum(x**2)\n",
    "            else:\n",
    "                # Regular numpy version\n",
    "                x = np.asarray(x)\n",
    "                return float(-0.5 * np.sum(x**2))\n",
    "\n",
    "        def jax_log_like(x):\n",
    "            return -0.5 * jnp.sum(x**2)\n",
    "\n",
    "        return log_like, jax_log_like\n",
    "\n",
    "    @staticmethod\n",
    "    def rosenbrock(ndim):\n",
    "        \"\"\"Rosenbrock banana test problem\"\"\"\n",
    "        def log_like(x):\n",
    "            if hasattr(x, 'type'):\n",
    "                # PyMC version\n",
    "                import pymc as pm\n",
    "                term1 = 100.0 * (x[1:] - x[:-1]**2.0)**2.0\n",
    "                term2 = (1 - x[:-1])**2.0\n",
    "                return -pm.math.sum(term1 + term2)\n",
    "            else:\n",
    "                # Regular numpy version\n",
    "                x = np.asarray(x)\n",
    "                return float(-np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0))\n",
    "\n",
    "        def jax_log_like(x):\n",
    "            return -jnp.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
    "\n",
    "        return log_like, jax_log_like\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_mixture(ndim):\n",
    "        \"\"\"Gaussian mixture test problem\"\"\"\n",
    "        def log_like(x):\n",
    "            if hasattr(x, 'type'):\n",
    "                # PyMC version\n",
    "                import pymc as pm\n",
    "                log_prob1 = -0.5 * pm.math.sum((x + 2)**2)\n",
    "                log_prob2 = -0.5 * pm.math.sum((x - 2)**2)\n",
    "                return pm.math.maximum(log_prob1, log_prob2)\n",
    "            else:\n",
    "                # Regular numpy version\n",
    "                x = np.asarray(x)\n",
    "                log_prob1 = -0.5 * np.sum((x + 2)**2)\n",
    "                log_prob2 = -0.5 * np.sum((x - 2)**2)\n",
    "                return float(np.logaddexp(log_prob1, log_prob2))\n",
    "\n",
    "        def jax_log_like(x):\n",
    "            log_prob1 = -0.5 * jnp.sum((x + 2)**2)\n",
    "            log_prob2 = -0.5 * jnp.sum((x - 2)**2)\n",
    "            return jnp.logaddexp(log_prob1, log_prob2)\n",
    "\n",
    "        return log_like, jax_log_like\n",
    "\n",
    "\n",
    "def measure_memory(func):\n",
    "    \"\"\"Decorator to measure memory usage\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem_before = memory_profiler.memory_usage()[0]\n",
    "        result = func(*args, **kwargs)\n",
    "        mem_after = memory_profiler.memory_usage()[0]\n",
    "        return result, mem_after - mem_before\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34cefe-5b6e-4556-9fab-918c74731706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb3500-f91a-4822-b4ee-e5a94325c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "### full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb99f9-29b6-4e68-b322-431305e0ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import pymc as pm\n",
    "import numpyro\n",
    "import dynesty\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import time\n",
    "import arviz as az\n",
    "import traceback\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "import threading\n",
    "import psutil\n",
    "\n",
    "class MemoryTracker:\n",
    "    def __init__(self, sampling_interval=0.1):\n",
    "        self.baseline_memory = None\n",
    "        self.peak_memory = {'rss': 0, 'vms': 0, 'shared': 0}\n",
    "        self.start_memory = None\n",
    "        self.end_memory = None\n",
    "        self.sampling_interval = sampling_interval\n",
    "        self.is_tracking = False\n",
    "        self.tracking_thread = None\n",
    "        \n",
    "    def _get_memory_usage(self):\n",
    "        current = psutil.Process()\n",
    "        memory_info = current.memory_info()\n",
    "        children_memory = {'rss': 0, 'vms': 0, 'shared': 0}\n",
    "        try:\n",
    "            for child in current.children(recursive=True):\n",
    "                try:\n",
    "                    child_mem = child.memory_info()\n",
    "                    children_memory['rss'] += child_mem.rss\n",
    "                    children_memory['vms'] += child_mem.vms\n",
    "                    children_memory['shared'] += getattr(child_mem, 'shared', 0)\n",
    "                except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                    continue\n",
    "        except psutil.Error:\n",
    "            pass\n",
    "            \n",
    "        return {\n",
    "            'rss': (memory_info.rss - self.baseline_memory.rss + children_memory['rss']) / (1024 * 1024),\n",
    "            'vms': (memory_info.vms - self.baseline_memory.vms + children_memory['vms']) / (1024 * 1024),\n",
    "            'shared': (getattr(memory_info, 'shared', 0) - getattr(self.baseline_memory, 'shared', 0) + \n",
    "                      children_memory['shared']) / (1024 * 1024),\n",
    "            'children_rss': children_memory['rss'] / (1024 * 1024),\n",
    "            'children_vms': children_memory['vms'] / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "    def _track_memory(self):\n",
    "        while self.is_tracking:\n",
    "            current_memory = self._get_memory_usage()\n",
    "            self.peak_memory['rss'] = max(self.peak_memory['rss'], current_memory['rss'])\n",
    "            self.peak_memory['vms'] = max(self.peak_memory['vms'], current_memory['vms'])\n",
    "            self.peak_memory['shared'] = max(self.peak_memory['shared'], current_memory['shared'])\n",
    "            time.sleep(self.sampling_interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.baseline_memory = psutil.Process().memory_info()\n",
    "        self.start_memory = self._get_memory_usage()\n",
    "        self.is_tracking = True\n",
    "        self.tracking_thread = threading.Thread(target=self._track_memory)\n",
    "        self.tracking_thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.is_tracking = False\n",
    "        if self.tracking_thread:\n",
    "            self.tracking_thread.join()\n",
    "        self.end_memory = self._get_memory_usage()\n",
    "        \n",
    "    def get_memory_stats(self):\n",
    "        return {\n",
    "            'peak': dict(self.peak_memory),\n",
    "            'change': {\n",
    "                'rss_change': self.end_memory['rss'] - self.start_memory['rss'],\n",
    "                'vms_change': self.end_memory['vms'] - self.start_memory['vms'],\n",
    "                'shared_change': self.end_memory['shared'] - self.start_memory['shared']\n",
    "            },\n",
    "            'children_peak': {\n",
    "                'rss': max(self.start_memory['children_rss'], \n",
    "                          self.end_memory['children_rss']),\n",
    "                'vms': max(self.start_memory['children_vms'], \n",
    "                          self.end_memory['children_vms'])\n",
    "            }\n",
    "        }\n",
    "\n",
    "def run_traditional_mcmc(log_like, ndim, param_ranges, ndraws=1000):\n",
    "    \"\"\"Run traditional MCMC using PyMC with improved tuning for complex distributions\"\"\"\n",
    "    active_mask = [True] * ndim  # All parameters are active in test problems\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        try:\n",
    "            # Define variables with proper ranges\n",
    "            params = []\n",
    "            for i in range(ndim):\n",
    "                min_val, max_val = param_ranges[i]\n",
    "                p = pm.Uniform(f'p_{i}', min_val, max_val)\n",
    "                params.append(p)\n",
    "            \n",
    "            params_combined = pm.math.stack(params)\n",
    "            likelihood = pm.Potential('likelihood', log_like(params_combined))\n",
    "            \n",
    "            # Use multiple step sizes for better adaptation\n",
    "            step_sizes = [0.1, 0.05, 0.01]\n",
    "            steps = []\n",
    "            for size in step_sizes:\n",
    "                steps.append(pm.Metropolis(\n",
    "                    vars=params,\n",
    "                    tune=True,\n",
    "                    scaling=size,\n",
    "                    tune_interval=50  # More frequent tuning (100)\n",
    "                ))\n",
    "            \n",
    "            # Start closer to the expected mode for each problem\n",
    "            start = {f'p_{i}': 0.5 * (param_ranges[i][0] + param_ranges[i][1]) \n",
    "                    for i in range(ndim)}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Increase tuning and sampling\n",
    "            trace = pm.sample(\n",
    "                draws=ndraws*5,  # More draws\n",
    "                tune=int(ndraws * 0.5),  # More tuning steps\n",
    "                return_inferencedata=True,\n",
    "                chains=4,\n",
    "                cores=4,\n",
    "                progressbar=False,\n",
    "                initvals=start,\n",
    "                step=steps,\n",
    "                discard_tuned_samples=True,\n",
    "#                target_accept=0.8  # Slightly higher acceptance rate target\n",
    "            )\n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            samples = np.stack([\n",
    "                trace.posterior[f'p_{i}'].values \n",
    "                for i in range(ndim)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            # Extract acceptance rate from trace\n",
    "            try:\n",
    "                if hasattr(trace, 'sample_stats'):\n",
    "                    accept_stat = trace.sample_stats.accept.mean().item()\n",
    "                    accept_rate = float(accept_stat * 100)  # Convert to percentage\n",
    "                else:\n",
    "                    accept_rate = None\n",
    "            except Exception as e:\n",
    "                print(f\"Could not extract acceptance rate: {e}\")\n",
    "                accept_rate = None\n",
    "            \n",
    "            return {\n",
    "                'trace': trace,\n",
    "                'samples': samples,\n",
    "                'raw_samples': samples,\n",
    "                'runtime': runtime,\n",
    "                'n_active': ndim,\n",
    "                'active_mask': active_mask,\n",
    "                'accept_rate': accept_rate\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"PyMC sampling error: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "def run_emcee(log_like, param_ranges, nwalkers=None, nsteps=1000):\n",
    "    \"\"\"Run emcee with robust error handling for test problems\"\"\"\n",
    "    ndim = len(param_ranges)\n",
    "    if nwalkers is None:\n",
    "        nwalkers = max(20 * ndim, 40)\n",
    "    \n",
    "    min_vals = np.array([r[0] for r in param_ranges])\n",
    "    max_vals = np.array([r[1] for r in param_ranges])\n",
    "    \n",
    "    def wrap_likelihood(x_phys):\n",
    "        \"\"\"Wrapper for physical-space likelihood\"\"\"\n",
    "        try:\n",
    "            if np.any(x_phys < min_vals) or np.any(x_phys > max_vals):\n",
    "                return -np.inf\n",
    "            return float(log_like(x_phys))\n",
    "        except Exception as e:\n",
    "            return -np.inf\n",
    "    \n",
    "    # Initialize walkers near the origin\n",
    "    pos = np.random.uniform(\n",
    "        low=min_vals + 0.1 * (max_vals - min_vals),\n",
    "        high=max_vals - 0.1 * (max_vals - min_vals),\n",
    "        size=(nwalkers, ndim)\n",
    "    )\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, \n",
    "        ndim, \n",
    "        wrap_likelihood,\n",
    "        a=2.0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        # Run burn-in\n",
    "        pos, prob, state = sampler.run_mcmc(pos, 100, progress=True)\n",
    "        sampler.reset()\n",
    "        \n",
    "        # Run production\n",
    "        final_pos, final_prob, final_state = sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "        \n",
    "        active_mask = [True] * ndim\n",
    "        samples = sampler.chain\n",
    "        \n",
    "        return {\n",
    "            'samples': np.expand_dims(samples, 0),\n",
    "            'raw_samples': samples,\n",
    "            'runtime': time.time() - start_time,\n",
    "            'accept_rate': np.mean(sampler.acceptance_fraction),\n",
    "            'log_prob': sampler.lnprobability,\n",
    "            'active_mask': active_mask,\n",
    "            'n_active': ndim\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during sampling: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_hmc(jax_log_like, ndim, param_ranges, num_warmup=500, num_samples=1000):\n",
    "    \"\"\"Run HMC using NumPyro for test problems\"\"\"\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random\n",
    "    import numpyro\n",
    "    import numpyro.distributions as dist\n",
    "    from numpyro.infer import MCMC, NUTS\n",
    "    \n",
    "    active_mask = [True] * ndim\n",
    "    \n",
    "    def model():\n",
    "        params = []\n",
    "        for i in range(ndim):\n",
    "            min_val, max_val = param_ranges[i]\n",
    "            # Use unconstrained sampler and transform\n",
    "            u = numpyro.sample(f'u_{i}', dist.Normal(0.0, 1.0))\n",
    "            param = numpyro.deterministic(\n",
    "                f'p_{i}',\n",
    "                min_val + (max_val - min_val) * jax.nn.sigmoid(u)\n",
    "            )\n",
    "            params.append(param)\n",
    "        \n",
    "        params = jnp.stack(params)\n",
    "        like = jax_log_like(params)\n",
    "        numpyro.factor('likelihood', like)\n",
    "        return params\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize the sampler\n",
    "        rng_key = random.PRNGKey(0)\n",
    "        kernel = NUTS(model, \n",
    "                     target_accept_prob=0.8,\n",
    "                     adapt_step_size=True,\n",
    "                     max_tree_depth=10)\n",
    "        \n",
    "        mcmc = MCMC(\n",
    "            kernel,\n",
    "            num_warmup=num_warmup,\n",
    "            num_samples=num_samples,\n",
    "            num_chains=1,\n",
    "            progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Run the sampler\n",
    "        mcmc.run(rng_key)\n",
    "        \n",
    "        # Get samples and transform them back to the original space\n",
    "        samples = mcmc.get_samples()\n",
    "        \n",
    "        # Extract parameter samples\n",
    "        active_samples = []\n",
    "        for i in range(ndim):\n",
    "            param_samples = np.array(samples[f'p_{i}'])\n",
    "            active_samples.append(param_samples)\n",
    "        \n",
    "        active_samples = np.stack(active_samples, axis=-1)\n",
    "        \n",
    "\n",
    "        if hasattr(mcmc, 'sampler') and hasattr(mcmc.sampler, 'acceptance_rate'):\n",
    "            accept_rate = float(mcmc.sampler.acceptance_rate)\n",
    "        else:\n",
    "           sample_stats = mcmc.get_extra_fields()\n",
    "        if 'accept_prob' in sample_stats:\n",
    "            accept_rate = float(np.mean(sample_stats['accept_prob']))\n",
    "        else:\n",
    "            print(\"Warning: Could not find acceptance rate in MCMC results\")\n",
    "            accept_rate = None\n",
    "        \n",
    "        return {\n",
    "            'samples': active_samples.reshape(1, -1, ndim),\n",
    "            'raw_samples': samples,\n",
    "            'runtime': time.time() - start_time,\n",
    "            'accept_rate': accept_rate,\n",
    "            'n_active': ndim,\n",
    "            'active_mask': active_mask\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during HMC sampling: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_nested(log_like, ndim, param_ranges, nlive=1000):\n",
    "    \"\"\"Run nested sampling using dynesty for test problems\"\"\"\n",
    "    def prior_transform(unit_coords):\n",
    "        \"\"\"Transform from unit cube to physical parameter space\"\"\"\n",
    "        physical_coords = np.zeros(ndim)\n",
    "        for i in range(ndim):\n",
    "            pmin, pmax = param_ranges[i]\n",
    "            physical_coords[i] = pmin + (pmax - pmin) * unit_coords[i]\n",
    "        return physical_coords\n",
    "    \n",
    "    sampler = dynesty.NestedSampler(\n",
    "        log_like,\n",
    "        prior_transform,\n",
    "        ndim,\n",
    "        nlive=nlive,\n",
    "        bound='multi',\n",
    "        sample='rwalk'\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sampler.run_nested(dlogz=0.25)\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    results = sampler.results\n",
    "    samples = results.samples\n",
    "    weights = np.exp(results.logwt - results.logz[-1])\n",
    "    samples_equal = dynesty.utils.resample_equal(samples, weights)\n",
    "    \n",
    "    active_mask = [True] * ndim\n",
    "    samples_reshaped = samples_equal.reshape(1, -1, ndim)\n",
    "    \n",
    "    return {\n",
    "        'samples': samples_reshaped,\n",
    "        'raw_samples': samples,\n",
    "        'runtime': runtime,\n",
    "        'accept_rate': results.eff,\n",
    "        'logz': results.logz[-1],\n",
    "        'weights': weights,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "\n",
    "def run_slice(log_like, ndim, param_ranges, ndraws=1000):\n",
    "    \"\"\"Run Slice Sampling for test problems\"\"\"\n",
    "    def slice_sample_step(x0, d, width=0.1):\n",
    "        x = x0.copy()\n",
    "        current_like = log_like(x)\n",
    "        slice_height = current_like - np.random.exponential()\n",
    "        \n",
    "        pmin, pmax = param_ranges[d]\n",
    "        r = np.random.uniform(0, width)\n",
    "        left = max(pmin, x[d] - r)\n",
    "        right = min(pmax, left + width)\n",
    "        left = max(pmin, right - width)\n",
    "        \n",
    "        while True:\n",
    "            x_left = x.copy()\n",
    "            x_left[d] = left\n",
    "            if log_like(x_left) <= slice_height or left <= pmin:\n",
    "                break\n",
    "            left = max(pmin, left - width)\n",
    "            \n",
    "        while True:\n",
    "            x_right = x.copy()\n",
    "            x_right[d] = right\n",
    "            if log_like(x_right) <= slice_height or right >= pmax:\n",
    "                break\n",
    "            right = min(pmax, right + width)\n",
    "            \n",
    "        while True:\n",
    "            new_x = np.random.uniform(left, right)\n",
    "            x_new = x.copy()\n",
    "            x_new[d] = new_x\n",
    "            \n",
    "            if log_like(x_new) > slice_height:\n",
    "                return x_new\n",
    "                \n",
    "            if new_x < x[d]:\n",
    "                left = new_x\n",
    "            else:\n",
    "                right = new_x\n",
    "\n",
    "    chains = 4\n",
    "    samples = np.zeros((chains, ndraws, ndim))\n",
    "    acceptance_rates = np.zeros(chains)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    active_mask = [True] * ndim\n",
    "    \n",
    "    for chain in range(chains):\n",
    "        x = np.zeros(ndim)\n",
    "        for i, (pmin, pmax) in enumerate(param_ranges):\n",
    "            x[i] = (pmin + pmax) / 2\n",
    "            \n",
    "        accepted = 0\n",
    "        \n",
    "        for i in range(ndraws):\n",
    "            x_old = x.copy()\n",
    "            \n",
    "            for d in range(ndim):\n",
    "                x = slice_sample_step(x, d)\n",
    "            \n",
    "            samples[chain, i] = x\n",
    "            if not np.array_equal(x, x_old):\n",
    "                accepted += 1\n",
    "                \n",
    "        acceptance_rates[chain] = accepted / ndraws\n",
    "    \n",
    "    return {\n",
    "        'samples': samples,\n",
    "        'raw_samples': samples,\n",
    "        'runtime': time.time() - start_time,\n",
    "        'accept_rate': np.mean(acceptance_rates),\n",
    "        'n_active': ndim,\n",
    "        'active_mask': active_mask\n",
    "    }\n",
    "\n",
    "\n",
    "def run_polychord(log_like, ndim, param_ranges, problem=\"test\", nlive=100):\n",
    "    \"\"\"Run nested sampling using PolyChord for test problems\"\"\"\n",
    "    import pypolychord\n",
    "    from pypolychord.settings import PolyChordSettings\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create chains directory if it doesn't exist\n",
    "    os.makedirs('chains', exist_ok=True)\n",
    "    \n",
    "    # Settings with dimension handling\n",
    "    settings = PolyChordSettings(ndim, 1)  # ndim parameters, 1 derived parameter (likelihood)\n",
    "    settings.nlive = nlive\n",
    "    settings.num_repeats = max(ndim * 5, 30)\n",
    "    #settings.file_root = f'polychord_run_simple_test_{ndim}dim'\n",
    "    settings.file_root = f'polychord_run_{problem}_{ndim}dim'\n",
    "    settings.base_dir = 'chains'\n",
    "    \n",
    "    # Clustering settings\n",
    "    settings.do_clustering = False\n",
    "    settings.cluster_posteriors = False\n",
    "    settings.boost_posterior = 1.0\n",
    "    settings.read_resume = False\n",
    "    \n",
    "    def prior(hypercube):\n",
    "        \"\"\"Transform unit hypercube to parameter space\"\"\"\n",
    "        from pypolychord.priors import UniformPrior\n",
    "        physical_params = []\n",
    "        for i, (pmin, pmax) in enumerate(param_ranges):\n",
    "            physical_params.append(UniformPrior(pmin, pmax)(hypercube[i]))\n",
    "        return physical_params\n",
    "    \n",
    "    def wrapped_log_like(theta):\n",
    "        \"\"\"Wrapper for likelihood to match PolyChord's interface\"\"\"\n",
    "        like = log_like(theta)\n",
    "        return like, [like]  # Return likelihood and derived parameters\n",
    "    \n",
    "    # Run sampler\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        output = pypolychord.run_polychord(\n",
    "            wrapped_log_like,\n",
    "            ndim, \n",
    "            1,  # One derived parameter (likelihood)\n",
    "            settings,\n",
    "            prior\n",
    "        )\n",
    "        runtime = time.time() - start_time\n",
    "        paramnames = [(f'p{i}', rf'p_{{{i}}}') for i in range(ndim )]\n",
    "        paramnames.append(('L', r'L'))  # Add the last parameter as 'L'\n",
    "        output.make_paramnames_files(paramnames)\n",
    "        posterior = output.posterior\n",
    "\n",
    "        samples_full = posterior.samples\n",
    "        samples = posterior.samples[:, :ndim]  # Exclude derived parameters\n",
    "        logZ, logZerr = output.logZ, output.logZerr\n",
    "\n",
    "        log_likelihoods = posterior['L']\n",
    "        logZ = output.logZ\n",
    "        logZe = output.logZerr\n",
    "        weights= log_likelihoods - logZ\n",
    "        \n",
    "\n",
    "       # samples_reshaped = samples.reshape(1, -1, ndim)\n",
    "        nlike = float(output.nlike) if hasattr(output, 'nlike') else None\n",
    "        npost = float(output.nposterior) if hasattr(output, 'nposterior') else None\n",
    "\n",
    "        equal_weights_file = f'chains/polychord_run_{problem}_{ndim}dim_equal_weights.txt'\n",
    "        try:\n",
    "            equal_weighted_samples = np.loadtxt(equal_weights_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read equal weights file: {str(e)}\")\n",
    "            equal_weighted_samples = None\n",
    "        \n",
    "        return {\n",
    "            'samples': samples, #np.expand_dims(samples, 0),  # Add chain dimension\n",
    "            'raw_samples': samples_full,\n",
    "            'weights': weights,\n",
    "            'equal_weighted_samples': equal_weighted_samples,\n",
    "            'runtime': float(runtime),\n",
    "            'accept_rate':float(nlike/npost) if (nlike is not None and npost is not None) else None,\n",
    "            'logz': float(output.logZ) if hasattr(output, 'logZ') else None,\n",
    "            'logz_err': float(output.logZerr) if hasattr(output, 'logZerr') else None,\n",
    "            'output': output\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PolyChord error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fedd9-1de7-4971-8819-7ce5444b9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_polychord_clust(log_like, ndim, param_ranges, problem=\"test\", nlive=100):\n",
    "    \"\"\"Run nested sampling using PolyChord for test problems\"\"\"\n",
    "    import pypolychord\n",
    "    from pypolychord.settings import PolyChordSettings\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create chains directory if it doesn't exist\n",
    "    os.makedirs('chains', exist_ok=True)\n",
    "    \n",
    "    # Settings with dimension handling\n",
    "    settings = PolyChordSettings(ndim, 1)  # ndim parameters, 1 derived parameter (likelihood)\n",
    "    settings.nlive = nlive\n",
    "    settings.num_repeats = max(ndim * 5, 30)\n",
    "    settings.file_root = f'polychord_run_clust_{problem}_{ndim}dim'\n",
    "    settings.base_dir = 'chains'\n",
    "    \n",
    "    # Clustering settings\n",
    "    settings.do_clustering = True\n",
    "    settings.cluster_posteriors = True\n",
    "    settings.boost_posterior = 1.0\n",
    "    settings.read_resume = False\n",
    "    \n",
    "    def prior(hypercube):\n",
    "        \"\"\"Transform unit hypercube to parameter space\"\"\"\n",
    "        from pypolychord.priors import UniformPrior\n",
    "        physical_params = []\n",
    "        for i, (pmin, pmax) in enumerate(param_ranges):\n",
    "            physical_params.append(UniformPrior(pmin, pmax)(hypercube[i]))\n",
    "        return physical_params\n",
    "    \n",
    "    def wrapped_log_like(theta):\n",
    "        \"\"\"Wrapper for likelihood to match PolyChord's interface\"\"\"\n",
    "        like = log_like(theta)\n",
    "        return like, [like]  \n",
    "    \n",
    "    # Run sampler\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        output = pypolychord.run_polychord(\n",
    "            wrapped_log_like,\n",
    "            ndim, \n",
    "            1,  # One derived parameter (likelihood)\n",
    "            settings,\n",
    "            prior\n",
    "        )\n",
    "        runtime = time.time() - start_time\n",
    "        paramnames = [(f'p{i}', rf'p_{{{i}}}') for i in range(ndim )]\n",
    "        paramnames.append(('L', r'L'))  # Add the last parameter as 'L'\n",
    "        output.make_paramnames_files(paramnames)\n",
    "        posterior = output.posterior\n",
    "\n",
    "        samples_full = posterior.samples\n",
    "        samples = posterior.samples[:, :ndim]  # Exclude derived parameters\n",
    "        logZ, logZerr = output.logZ, output.logZerr\n",
    "\n",
    "        log_likelihoods = posterior['L']\n",
    "        logZ = output.logZ\n",
    "        logZe = output.logZerr\n",
    "        weights= log_likelihoods - logZ\n",
    "        \n",
    "\n",
    "       # samples_reshaped = samples.reshape(1, -1, ndim)\n",
    "        nlike = float(output.nlike) if hasattr(output, 'nlike') else None\n",
    "        npost = float(output.nposterior) if hasattr(output, 'nposterior') else None\n",
    "\n",
    "        equal_weights_file = f'chains/polychord_run_clust_{problem}_{ndim}dim_equal_weights.txt'\n",
    "        try:\n",
    "            equal_weighted_samples = np.loadtxt(equal_weights_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read equal weights file: {str(e)}\")\n",
    "            equal_weighted_samples = None\n",
    "        \n",
    "        return {\n",
    "            'samples': samples,\n",
    "            'raw_samples': samples_full,\n",
    "            'weights': weights,\n",
    "            'equal_weighted_samples': equal_weighted_samples,\n",
    "            'runtime': float(runtime),\n",
    "            'accept_rate':float(nlike/npost) if (nlike is not None and npost is not None) else None,\n",
    "            'logz': float(output.logZ) if hasattr(output, 'logZ') else None,\n",
    "            'logz_err': float(output.logZerr) if hasattr(output, 'logZerr') else None,\n",
    "            'output': output\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PolyChord error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8425bf3-1f07-40f8-8bb0-b6edd7af27a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa97904-edd9-4da8-bebb-5f5f0ec278bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3dc58-d444-425c-b0de-fd9ea08ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Samplers class\n",
    "class Samplers:\n",
    "    pass\n",
    "\n",
    "# Add `run_traditional_mcmc` as a static method to `Samplers`\n",
    "Samplers.run_traditional_mcmc = staticmethod(measure_memory(run_traditional_mcmc))\n",
    "Samplers.run_emcee = staticmethod(measure_memory(run_emcee))\n",
    "Samplers.run_hmc = staticmethod(measure_memory(run_hmc))\n",
    "Samplers.run_nested = staticmethod(measure_memory(run_nested))\n",
    "Samplers.run_slice = staticmethod(measure_memory(run_slice))\n",
    "Samplers.run_polychord = staticmethod(measure_memory(run_polychord))\n",
    "Samplers.run_polychord_clust = staticmethod(measure_memory(run_polychord_clust))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b27c72-aebe-446d-81ca-618bd14d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339648b7-a5cb-4a41-ae1b-5a82beccacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diagnostics(samples, method, weights=None):\n",
    "    \"\"\"Calculate diagnostics based on sampler type\"\"\"\n",
    "    try:\n",
    "        if method in ['traditional', 'emcee', 'hmc', 'slice']:\n",
    "            if samples is not None:\n",
    "                try:\n",
    "                    # Convert samples to Arviz format if needed\n",
    "                    if not isinstance(samples, az.InferenceData):\n",
    "                        # Reshape samples to ensure minimum shape requirements\n",
    "                        if len(samples.shape) == 2:  # (samples, params)\n",
    "                            samples = samples.reshape(2, -1, samples.shape[-1])  # Split into 2 chains\n",
    "                        samples = az.convert_to_dataset(samples)\n",
    "                    ess = az.ess(samples)\n",
    "                    r_hat = az.rhat(samples)\n",
    "                    \n",
    "                    # Handle different types of returns\n",
    "                    if isinstance(ess, dict):\n",
    "                        ess_values = [val for val in ess.values() if not np.isnan(val).any()]\n",
    "                    elif hasattr(ess, 'to_dataarray'):\n",
    "                        ess_values = [float(val) for val in ess.to_dataarray().values.flatten() \n",
    "                                    if not np.isnan(val)]\n",
    "                    else:\n",
    "                        ess_values = [val for val in np.array(ess).flatten() if not np.isnan(val)]\n",
    "                    ess_mean = float(np.mean(ess_values)) if ess_values else None\n",
    "                    \n",
    "                    # Similar for r_hat\n",
    "                    if isinstance(r_hat, dict):\n",
    "                        r_hat_values = [val for val in r_hat.values() if not np.isnan(val).any()]\n",
    "                    elif hasattr(r_hat, 'to_dataarray'):\n",
    "                        r_hat_values = [float(val) for val in r_hat.to_dataarray().values.flatten() \n",
    "                                      if not np.isnan(val)]\n",
    "                    else:\n",
    "                        r_hat_values = [val for val in np.array(r_hat).flatten() if not np.isnan(val)]\n",
    "                    r_hat_mean = float(np.mean(r_hat_values)) if r_hat_values else None\n",
    "                    \n",
    "                    return ess_mean, r_hat_mean\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to calculate MCMC diagnostics: {str(e)}\")\n",
    "                    return None, None\n",
    "        elif method in ['nested', 'polychord', 'polychord_clust']:\n",
    "            # Special handling for nested samplers\n",
    "            if weights is not None:\n",
    "                try:\n",
    "                    normalized_weights = weights / np.sum(weights)\n",
    "                    ess = 1.0 / np.sum(normalized_weights ** 2)\n",
    "                    return float(ess), None\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to calculate {method} ESS: {str(e)}\")\n",
    "                    return None, None\n",
    "            else:\n",
    "                print(f\"No weights provided for {method}\")\n",
    "                return None, None\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Diagnostic calculation failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def run_benchmark(problems=[\"gaussian\", \"rosenbrock\", \"mixture\"], \n",
    "                 dims=[2, 5, 10],\n",
    "                 methods=['traditional', 'emcee', 'hmc', 'nested', 'slice', 'polychord', 'polychord_clust']):\n",
    "    \"\"\"\n",
    "    Run benchmarks on test problems\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    problem_funcs = {\n",
    "        \"gaussian\": TestProblems.correlated_gaussian,\n",
    "        \"rosenbrock\": TestProblems.rosenbrock,\n",
    "        \"mixture\": TestProblems.gaussian_mixture\n",
    "    }\n",
    "    \n",
    "    default_ranges = {\n",
    "        \"gaussian\": (-5, 5),\n",
    "        \"rosenbrock\": (-5, 5),\n",
    "        \"mixture\": (-5, 5)\n",
    "    }\n",
    "    \n",
    "    for problem_name in problems:\n",
    "        results[problem_name] = {\n",
    "            'dims': dims,\n",
    "            'samples': {method: [] for method in methods},\n",
    "            'raw_samples': {method: [] for method in methods},\n",
    "            'metrics': {\n",
    "                'runtime': {method: [] for method in methods},\n",
    "                'ess_per_sec': {method: [] for method in methods},\n",
    "                'accuracy': {method: [] for method in methods},\n",
    "                'accept_rate': {method: [] for method in methods},\n",
    "                'r_hat': {method: [] for method in methods},\n",
    "                'memory_usage': {method: [] for method in methods},\n",
    "                'init_sensitivity': {method: [] for method in methods},\n",
    "                'weights': {method: [] for method in methods},\n",
    "                'raw_weights': {method: [] for method in methods},\n",
    "                'log_weights': {method: [] for method in methods}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBenchmarking {problem_name} distribution...\")\n",
    "        \n",
    "        for d in dims:\n",
    "            print(f\"\\nDimension: {d}\")\n",
    "            \n",
    "            log_like, jax_log_like = problem_funcs[problem_name](d)\n",
    "            range_min, range_max = default_ranges[problem_name]\n",
    "            param_ranges = [(range_min, range_max) for _ in range(d)]\n",
    "            \n",
    "            # Set true parameters for each problem type\n",
    "            if problem_name == \"gaussian\":\n",
    "                true_params = np.zeros(d)\n",
    "            elif problem_name == \"rosenbrock\":\n",
    "                true_params = np.ones(d)\n",
    "            else:  # mixture\n",
    "                true_params = np.array([2.0] * d)  # One of the modes\n",
    "            \n",
    "            for method in methods:\n",
    "                print(f\"\\nRunning {method}...\")\n",
    "                try:\n",
    "                    init_results = []\n",
    "                    all_memory_stats = []\n",
    "                    result = None\n",
    "                    \n",
    "                    for init_run in range(3):\n",
    "                        memory_tracker = MemoryTracker()\n",
    "                        memory_tracker.start()\n",
    "                        \n",
    "                        try:\n",
    "                            if method == 'traditional':\n",
    "                                result_i = run_traditional_mcmc(log_like, d, param_ranges)\n",
    "                            elif method == 'emcee':\n",
    "                                result_i = run_emcee_robust(log_like, param_ranges)\n",
    "                            elif method == 'hmc':\n",
    "                                result_i = run_hmc(jax_log_like, d, param_ranges)\n",
    "                            elif method == 'slice':\n",
    "                                result_i = run_slice(log_like, d, param_ranges)\n",
    "                            elif method == 'nested':\n",
    "                                result_i = run_nested(log_like, d, param_ranges)\n",
    "                            elif method == 'polychord':\n",
    "                                result_i = run_polychord(log_like, d, param_ranges, problem=problem_name)\n",
    "                            elif method == 'polychord_clust':\n",
    "                                result_i = run_polychord_clust(log_like, d, param_ranges, problem=problem_name)\n",
    "                            \n",
    "                            memory_tracker.stop()\n",
    "                            memory_stats = memory_tracker.get_memory_stats()\n",
    "                            all_memory_stats.append(memory_stats)\n",
    "                            \n",
    "                            if result_i is None:\n",
    "                                raise ValueError(f\"{method} returned None result\")\n",
    "                            \n",
    "                            if init_run == 0:\n",
    "                                result = result_i\n",
    "                            \n",
    "                            samples_i = result_i['samples']\n",
    "                            samples_flat_i = samples_i.reshape(-1, d)\n",
    "                            init_results.append(np.mean(samples_flat_i, axis=0))\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in initialization run {init_run}: {str(e)}\")\n",
    "                            result_i = None\n",
    "                            memory_tracker.stop()\n",
    "                            break\n",
    "                    \n",
    "                    init_sensitivity = float(np.std(init_results, axis=0).mean()) if init_results else None\n",
    "                    \n",
    "                    if result is None:\n",
    "                        raise ValueError(f\"No valid result from {method}\")\n",
    "                    \n",
    "                    if all_memory_stats:\n",
    "                        memory = {\n",
    "                            'peak': {\n",
    "                                'rss': max(m['peak']['rss'] for m in all_memory_stats),\n",
    "                                'vms': max(m['peak']['vms'] for m in all_memory_stats),\n",
    "                                'shared': max(m['peak']['shared'] for m in all_memory_stats)\n",
    "                            },\n",
    "                            'change': {\n",
    "                                'rss_change': np.mean([m['change']['rss_change'] for m in all_memory_stats]),\n",
    "                                'vms_change': np.mean([m['change']['vms_change'] for m in all_memory_stats]),\n",
    "                                'shared_change': np.mean([m['change']['shared_change'] for m in all_memory_stats])\n",
    "                            },\n",
    "                            'children_peak': {\n",
    "                                'rss': max(m['children_peak']['rss'] for m in all_memory_stats),\n",
    "                                'vms': max(m['children_peak']['vms'] for m in all_memory_stats)\n",
    "                            }\n",
    "                        }\n",
    "                    else:\n",
    "                        memory = {\n",
    "                            'peak': {'rss': 0, 'vms': 0, 'shared': 0},\n",
    "                            'change': {'rss_change': 0, 'vms_change': 0, 'shared_change': 0},\n",
    "                            'children_peak': {'rss': 0, 'vms': 0}\n",
    "                        }\n",
    "                    \n",
    "                    samples = result['samples']\n",
    "                    samples_flat = samples.reshape(-1, d)\n",
    "                    sample_means = np.mean(samples_flat, axis=0)\n",
    "                    sample_std = np.std(samples_flat, axis=0)\n",
    "                    \n",
    "                    # Create inference data for diagnostics\n",
    "                    if method in ['traditional', 'emcee', 'hmc', 'slice']:\n",
    "                        try:\n",
    "                            # Ensure proper shape for arviz\n",
    "                            n_samples = len(samples_flat)\n",
    "                            chain_samples = samples_flat.reshape(2, n_samples//2, d)  # Split into 2 chains\n",
    "                            samples_az = az.convert_to_inference_data(\n",
    "                                {\"parameter\": chain_samples},\n",
    "                                group=\"posterior\"\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to convert samples to InferenceData: {e}\")\n",
    "                            samples_az = None\n",
    "                    else:\n",
    "                        samples_az = None\n",
    "                    \n",
    "                    weights = result.get('weights', None)\n",
    "                    ess_mean, r_hat_mean = calculate_diagnostics(samples_az, method, weights)\n",
    "                    \n",
    "                    results[problem_name]['raw_samples'][method].append(result.get('raw_samples', None))\n",
    "                    results[problem_name]['samples'][method].append(samples_flat)\n",
    "                    \n",
    "                    metrics = results[problem_name]['metrics']\n",
    "                    metrics['runtime'][method].append(result['runtime'])\n",
    "                    metrics['ess_per_sec'][method].append(None if ess_mean is None else ess_mean/result['runtime'])\n",
    "                    metrics['accept_rate'][method].append(result.get('accept_rate', None))\n",
    "                    metrics['r_hat'][method].append(r_hat_mean)\n",
    "                    metrics['memory_usage'][method].append(memory)\n",
    "                    metrics['init_sensitivity'][method].append(init_sensitivity)\n",
    "                    metrics['weights'][method].append(weights)\n",
    "                    metrics['raw_weights'][method].append(result.get('weights', None))\n",
    "                    metrics['log_weights'][method].append(result.get('equal_weighted_samples', None))\n",
    "\n",
    "                    # Calculate accuracy using true_params\n",
    "                    parameter_deviations = np.abs(sample_means - true_params)\n",
    "                    accuracy_metrics = {\n",
    "                        'average_deviation': float(np.mean(parameter_deviations)),\n",
    "                        'parameter_deviations': [float(x) for x in parameter_deviations],\n",
    "                        'true_values': [float(x) for x in true_params],\n",
    "                        'sampled_means': [float(x) for x in sample_means],\n",
    "                        'sampled_std': [float(x) for x in sample_std]\n",
    "                    }\n",
    "                    metrics['accuracy'][method].append(accuracy_metrics)\n",
    "\n",
    "                    print(f\"Successfully stored results for {method}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error running {method} on {problem_name} in {d} dimensions:\")\n",
    "                    print(f\"Error message: {str(e)}\")\n",
    "                    print(f\"Error type: {type(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    for metric in results[problem_name]['metrics']:\n",
    "                        results[problem_name]['metrics'][metric][method].append(None)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9057a313-acf9-452c-9d5a-a32f22c8d1bb",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa0697-9f8b-4cbe-8532-f0dcd8db162b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full run of all the samplers\n",
    "results_finalC = run_benchmark(\n",
    "    problems=[\"gaussian\", \"rosenbrock\", \"mixture\"],#\n",
    "    dims=[2, 3, 4, 6, 8, 10], #\n",
    "    methods=['traditional', 'emcee','hmc' , 'nested', 'slice', 'polychord'] #\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220e278-9d74-4d94-8379-47bee67a1312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f917194-71f1-4f1f-b56e-7caa4b7e8024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dac2a-d19a-400a-b115-d26cc8af19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(results, save_plot=True):\n",
    "    \"\"\"Create and save a summary plot of key metrics\"\"\"\n",
    "    problems = list(results.keys())\n",
    "    methods = list(results[problems[0]]['metrics']['runtime'].keys())\n",
    "    key_metrics = ['runtime', 'memory_usage', 'ess_per_sec', 'init_sensitivity']  #'accept_rate'\n",
    "    colors = {'traditional': '#ff6b6b', 'emcee': '#9370db',\n",
    "              'hmc': 'green', 'nested': '#45b7d1', 'slice': '#ffd93d', 'polychord': 'black', 'polychord_clust': 'gray'}\n",
    "    line_styles = ['solid', 'dashed', 'dashdot']  # Cycle through these styles for problems\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15), dpi=300)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    legend_entries = []\n",
    "    legend_lines = []\n",
    "    legend_labels = []\n",
    "\n",
    "\n",
    "    for idx, metric in enumerate(key_metrics):\n",
    "        ax = axes[idx]\n",
    "        for problem_idx, problem in enumerate(problems):\n",
    "            for method in methods:\n",
    "                dims = results[problem]['dims']\n",
    "                values = results[problem]['metrics'][metric][method]\n",
    "\n",
    "                if metric == 'memory_usage':\n",
    "                    mem_data = [v if isinstance(v, dict) else None for v in values]\n",
    "                    rss_values = [abs(m['peak']['rss'])/1000 if m else np.nan for m in mem_data]\n",
    "                    vms_values = [abs(m['peak']['vms'])/1000 if m else np.nan for m in mem_data]\n",
    "\n",
    "                    rss_values = np.array(rss_values)\n",
    "                    vms_values = np.array(vms_values)\n",
    "                    valid_mask = ~np.isnan(rss_values)\n",
    "\n",
    "                    valid_dims = np.array(dims)[valid_mask]\n",
    "                    valid_rss = rss_values[valid_mask]\n",
    "                    valid_vms = vms_values[valid_mask]\n",
    "\n",
    "                    sort_idx = np.argsort(valid_dims)\n",
    "                    valid_dims = valid_dims[sort_idx]\n",
    "                    valid_rss = valid_rss[sort_idx]\n",
    "                    valid_vms = valid_vms[sort_idx]\n",
    "\n",
    "                    if len(valid_rss) > 0:\n",
    "                       # rss_line = ax.plot(valid_dims, valid_rss, \n",
    "                       #                    linestyle=line_styles[problem_idx % len(line_styles)], \n",
    "                       #                    color=colors[method],\n",
    "                       #                    marker='o', label=f'{method} ({problem}, RSS)')[0]\n",
    "                       # legend_entries.append(rss_line)\n",
    "\n",
    "                        vms_line = ax.plot(valid_dims, valid_vms, \n",
    "                                           linestyle=line_styles[problem_idx % len(line_styles)], \n",
    "                                           color=colors[method],\n",
    "                                           marker='s', label=f'{method} ({problem}, VMS)')[0]\n",
    "                        legend_entries.append(vms_line)\n",
    "\n",
    "                else:\n",
    "                    valid_dims = [d for d, v in zip(dims, values) if v is not None]\n",
    "                    valid_values = [abs(v) if v is not None else np.nan for v in values]\n",
    "                    if valid_values:\n",
    "                        line = ax.plot(valid_dims, valid_values, \n",
    "                                       linestyle=line_styles[problem_idx % len(line_styles)],\n",
    "                                       color=colors[method], marker='o',\n",
    "                                       label=f'{method} ({problem})')[0]\n",
    "                        legend_entries.append(line)\n",
    "\n",
    "                        # Store for legend if this is the first metric\n",
    "                        if idx == 0:\n",
    "                            legend_lines.append(line)\n",
    "                            legend_labels.append(f'{method} ({problem})')\n",
    "\n",
    "        ax.set_xlabel('Dimensions')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        if metric == 'ess_per_sec':\n",
    "            ax.set_ylabel('ESS per sec')\n",
    "            ax.set_title('ESS per sec')\n",
    "        if metric in ['ess_per_sec', 'accuracy', 'memory_usage']:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "    # Add legend for all subplots\n",
    "    ax_handles, ax_labels = ax.get_legend_handles_labels()\n",
    "    unique_handles_labels = list(dict(zip(ax_labels, ax_handles)).items())\n",
    "    handles, labels = zip(*unique_handles_labels)\n",
    "\n",
    "    axes[0].legend(legend_lines, legend_labels,\n",
    "                   loc='center left', bbox_to_anchor=(-0.01, 0.665), fontsize=\"11\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    if save_plot:\n",
    "        filename = 'summary_comparison_T1000_clust.pdf'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filename}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98de1e-3486-4e25-9032-81b6a4b0031b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a28d9-c6a9-4f31-9eaf-619b54d66563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26d7b7-91c8-49bb-bafe-7c326d557a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary(results_finalC) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ab344-78a1-4537-9901-a89b35110748",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a069495-1e2e-4dc8-8a5a-cdbb768e12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "results_original = copy.deepcopy(results_finalC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eed0e-1a90-4e8c-ba34-194acd3a7fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81cc8a-1e39-4be7-8c95-cca06ca214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def calculate_distribution_accuracy(samples, problem_type, ndim, weights=None, method=None):\n",
    "    \"\"\"Calculate both mean-based and distribution-specific accuracy metrics\"\"\"\n",
    "    if samples is None:\n",
    "        return None, None\n",
    "        \n",
    "    # Make a copy to avoid modifying original data\n",
    "    samples = samples.copy()\n",
    "    \n",
    "    # Check dimensions match expected\n",
    "    if samples.shape[1] != ndim:\n",
    "        print(f\"Warning: Sample dimension ({samples.shape[1]}) doesn't match expected ndim ({ndim})\")\n",
    "    \n",
    "    # Current mean-based metric\n",
    "    if problem_type == 'gaussian':\n",
    "        true_mode = np.zeros(ndim)\n",
    "        if weights is not None:\n",
    "            sample_mean = np.average(samples, weights=weights, axis=0)\n",
    "        else:\n",
    "            sample_mean = np.mean(samples, axis=0)\n",
    "        mean_metric = np.sqrt(np.mean((sample_mean - true_mode)**2))\n",
    "        print(f\"Gaussian metric - mean: {sample_mean}, metric: {mean_metric}\")  # Debug\n",
    "        dist_metric = mean_metric\n",
    "        \n",
    "    elif problem_type == 'rosenbrock':\n",
    "        # Mean-based metric\n",
    "        true_mode = np.ones(ndim)\n",
    "        if weights is not None:\n",
    "            sample_mean = np.average(samples, weights=weights, axis=0)\n",
    "        else:\n",
    "            sample_mean = np.mean(samples, axis=0)\n",
    "        mean_metric = np.sqrt(np.mean((sample_mean - true_mode)**2))\n",
    "        \n",
    "        # Distribution-based metric\n",
    "        errors = []\n",
    "        for sample in samples:\n",
    "            error = np.mean(100.0 * (sample[1:] - sample[:-1]**2.0)**2.0 + \n",
    "                          (1 - sample[:-1])**2.0)\n",
    "            errors.append(error)\n",
    "        if weights is not None:\n",
    "            dist_metric = np.average(errors, weights=weights)\n",
    "        else:\n",
    "            dist_metric = np.mean(errors)\n",
    "        \n",
    "    elif problem_type == 'mixture':\n",
    "        # Mean-based metric using KMeans\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        if weights is not None:\n",
    "            kmeans.fit(samples, sample_weight=weights)\n",
    "        else:\n",
    "            kmeans.fit(samples)\n",
    "            \n",
    "        found_modes = kmeans.cluster_centers_\n",
    "        true_modes = np.array([2.0 * np.ones(ndim), -2.0 * np.ones(ndim)])\n",
    "        \n",
    "        # Calculate distances to true modes, accounting for symmetry\n",
    "        dist1 = np.min([\n",
    "            np.sqrt(np.mean((found_modes[0] - true_modes[0])**2)),\n",
    "            np.sqrt(np.mean((found_modes[0] - true_modes[1])**2))\n",
    "        ])\n",
    "        dist2 = np.min([\n",
    "            np.sqrt(np.mean((found_modes[1] - true_modes[0])**2)),\n",
    "            np.sqrt(np.mean((found_modes[1] - true_modes[1])**2))\n",
    "        ])\n",
    "        \n",
    "        # Get cluster weights\n",
    "        if weights is not None:\n",
    "            labels = kmeans.predict(samples)\n",
    "            weights_by_cluster = [np.sum(weights[labels == i]) for i in range(2)]\n",
    "            weights_by_cluster = np.array(weights_by_cluster) / np.sum(weights)\n",
    "        else:\n",
    "            weights_by_cluster = np.array([np.mean(kmeans.labels_ == i) for i in range(2)])\n",
    "            \n",
    "        weight_balance = abs(weights_by_cluster[0] - 0.5)\n",
    "        mean_metric = float(dist1 + dist2 + weight_balance)\n",
    "        \n",
    "        # Distribution-based metric\n",
    "        log_probs = []\n",
    "        for sample in samples:\n",
    "            log_prob1 = -0.5 * np.sum((sample + 2)**2)\n",
    "            log_prob2 = -0.5 * np.sum((sample - 2)**2)\n",
    "            log_probs.append(np.logaddexp(log_prob1, log_prob2))\n",
    "        \n",
    "        if weights is not None:\n",
    "            dist_metric = -np.average(log_probs, weights=weights)\n",
    "        else:\n",
    "            dist_metric = -np.mean(log_probs)\n",
    "    \n",
    "    return mean_metric, dist_metric\n",
    "def update_accuracy_metrics_comparison(results):\n",
    "    \"\"\"Update results with both types of accuracy metrics\"\"\"\n",
    "    for problem_name in results.keys():\n",
    "        dims = results[problem_name]['dims']\n",
    "        \n",
    "        if 'distribution_accuracy' not in results[problem_name]['metrics']:\n",
    "            results[problem_name]['metrics']['distribution_accuracy'] = {}\n",
    "        \n",
    "        for method in results[problem_name]['samples'].keys():\n",
    "            mean_accuracy = []\n",
    "            dist_accuracy = []\n",
    "            \n",
    "            for i, d in enumerate(dims):\n",
    "                # For PolyChord, use equal-weighted samples and correct columns\n",
    "                if method == 'polychord' or method == 'polychord_clust':\n",
    "                    full_samples = results[problem_name]['metrics']['log_weights'][method][i]\n",
    "                    samples = full_samples[:, 2:2+d]  # d is the number of parameters\n",
    "                    weights = None  # No weights needed for equal-weighted samples\n",
    "                else:\n",
    "                    samples = results[problem_name]['samples'][method][i]\n",
    "                    weights = results[problem_name]['metrics']['weights'][method][i] if method in ['nested'] else None\n",
    "                    if samples is not None and len(samples.shape) == 3:\n",
    "                        samples = samples.reshape(-1, samples.shape[-1])\n",
    "                \n",
    "                if samples is not None:\n",
    "                    mean_metric, dist_metric = calculate_distribution_accuracy(\n",
    "                        samples, problem_name, d, weights, method\n",
    "                    )\n",
    "                    mean_accuracy.append({'value': mean_metric})\n",
    "                    dist_accuracy.append({'value': dist_metric})\n",
    "                else:\n",
    "                    mean_accuracy.append({'value': None})\n",
    "                    dist_accuracy.append({'value': None})\n",
    "            \n",
    "            results[problem_name]['metrics']['accuracy'][method] = mean_accuracy\n",
    "            results[problem_name]['metrics']['distribution_accuracy'][method] = dist_accuracy\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c668b4-8254-436b-9bc9-71dcd84606be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe9584-db9a-40c1-b017-303fe594e966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_resultsX00 = update_accuracy_metrics_comparison(results_finalC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad46ba5-2778-4aa0-88da-54c3dc81fe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0da25-b0f3-414d-9bd0-5847724584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final function used for the plots in the paper\n",
    "def plot_accuracy_comparison_final(results):\n",
    "    \"\"\"Plot accuracy metrics using fixed scale normalization\"\"\"\n",
    "    problems = list(results.keys())\n",
    "    methods = list(results[problems[0]]['metrics']['runtime'].keys())\n",
    "    \n",
    "    # Color scheme for methods\n",
    "    colors = {\n",
    "        'traditional': '#ff6b6b', \n",
    "        'emcee': '#9370db',\n",
    "        'hmc': 'green', ##4ecdc4', \n",
    "        'nested': '#45b7d1', \n",
    "        'slice': '#ffd93d', \n",
    "        'polychord': 'black',\n",
    "        'polychord_clust': 'gray'\n",
    "    }\n",
    "    \n",
    "    # Line styles for different problems\n",
    "    line_styles = {\n",
    "        problems[0]: 'solid',        # gaussian\n",
    "        problems[1]: 'dashed',       # rosenbrock\n",
    "        problems[2]: 'dashdot'       # mixture\n",
    "    }\n",
    "    \n",
    "    # Create figure with mean-based and distribution-based metrics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    metrics = ['accuracy', 'distribution_accuracy']\n",
    "    titles = ['Mean-based Accuracy', 'Distribution-based Accuracy']\n",
    "    \n",
    "    max_errors = {\n",
    "        'gaussian': 0.5,\n",
    "        'rosenbrock': 2.0,\n",
    "        'mixture': 5.0  # Adjusted to match the scale of mixture distribution metric\n",
    "    }\n",
    "    \n",
    "    dims = results[problems[0]]['dims']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for method in methods:\n",
    "            for problem in problems:\n",
    "                values = results[problem]['metrics'][metric][method]\n",
    "                \n",
    "                normalized_values = []\n",
    "                valid_dims = []\n",
    "                \n",
    "                for d_idx, d in enumerate(dims):\n",
    "                    value = values[d_idx]\n",
    "                    value = value['value'] if isinstance(value, dict) else value\n",
    "                    \n",
    "                    if value is not None:\n",
    "                        # Apply fixed scale normalization\n",
    "                        max_error = max_errors[problem]\n",
    "                        norm_value = 1 - min(value / max_error, 1.0)  # Clip at 0\n",
    "                        normalized_values.append(norm_value)\n",
    "                        valid_dims.append(d)\n",
    "                \n",
    "                if normalized_values:\n",
    "                    ax.plot(valid_dims, normalized_values,\n",
    "                           linestyle=line_styles[problem],\n",
    "                           color=colors[method],\n",
    "                           marker='o',\n",
    "                           label=f'{method} ({problem})')\n",
    "        \n",
    "        ax.set_xlabel('Dimensions')\n",
    "        ax.set_ylabel('Normalized Accuracy (higher is better)')\n",
    "        ax.set_ylim(-0.01, 1.03)  # Add some padding\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        if idx == 0:  \n",
    "            ax.set_ylim(-0.01, 1.03) \n",
    "            #ax.set_yscale('log')  # Apply log scale to first plot only\n",
    "            ax.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "        \n",
    "        ax.grid(True, linestyle='--', alpha=0.7)  # Grid for readability\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('accuracy_comparison_final.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b10e1-6789-41ab-b74b-944ad23853ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348da235-42aa-4c2a-a3b0-1601558b7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_comparison_final(updated_resultsX00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCMC_env",
   "language": "python",
   "name": "mcmc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
